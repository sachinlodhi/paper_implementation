# -*- coding: utf-8 -*-
"""paper3_cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JwqWkRM2Gjn4-R1VE5exrG1357OoZmKb
"""

import numpy as np
import random

class lenet5:

  ''' initialization stuff '''
  def __init__(self, learning_rate =0.1, epochs= 20):
    self.lr  = learning_rate
    self.epochs = epochs

    '''for conv layers: (output_channels, input_channels, kernel_height, kernel_width)'''
    '''block 1'''
    # layer 1( Convolution 1) -> 6 ouptut feature maps with 5x5 kernel: 32x32 input and 28x28 output
    self.num_conv1_filters = 6 # filters
    self.conv1_kernel_size = 5 # kernel (this detects and extract the features)
    # as last time I did it manually this time I am going to intialize the weights using function so can break the symmetery
    self.conv1_weights = np.random.randn(self.num_conv1_filters, 1, self.conv1_kernel_size, self.conv1_kernel_size) * 0.1 # 6 filters with, 1 channel (gray i.e. input image), 2d filter, x0.1 to scale down vals to prevent grad exploding
    self.conv1_bias = np.zeros(self.num_conv1_filters)

    # layer 2(oversampling/subsampling): 6 feature map with 2x2 sized so 28x28(conv output) -> 14x14(sampling output)
    self.pool1_size = 2
    self.pool1_weights = np.random.randn(self.num_conv1_filters) * 0.1
    self.pool1_bias = np.zeros(self.num_conv1_filters) # almost missed it lol


    '''block 2'''
    # layer 3 (convolution 2): 14x14 -> 10x10 @ 16 filters of size 5x5
    self.num_conv2_filters = 16
    self.conv2_kernel_size = 5
    # numebr of filters(output), the input it is receiving from the previous layer i.e. 6 inputs from the previous layer, height of kernel, width of kernel, 0*1= bump the initialization down
    self.conv2_weights = np.random.randn(self.num_conv2_filters,self.num_conv1_filters, self.conv2_kernel_size, self.conv2_kernel_size) * 0.1
    self.conv2_bias = np.zeros(self.num_conv2_filters)

    # layer 4 (sampling): 10x10 -> 5x5 @ 16 filters of size 2x2
    self.pool2_size = 2
    self.pool2_weights = np.random.randn(self.num_conv2_filters) * 0.1
    self.pool2_bias = np.zeros(self.num_conv2_filters)

    ''' block 3 '''
    # layer 5
    # convolutional layer : 120(manually defined) feature map. Each feature map is of size 5x5(as output from the previous layer)
    self.num_conv3_filters = 120
    self.conv3_kernel_size = 5 # same as the previous one
    self.conv3_weights = np.random.randn(self.num_conv3_filters,self.num_conv2_filters, self.conv3_kernel_size, self.conv3_kernel_size ) * 0.1
    self.conv3_bias = np.zeros(self.num_conv3_filters)

    '''for fully connected layers: (input_features, output_features)'''
    # layer 6
    # fully connected layer: 84 units
    self.num_fc1_filters = 84
    self.fc1_weights = np.random.randn(self.num_conv3_filters, self.num_fc1_filters) * 0.1
    self.fc1_bias = np.zeros(self.num_fc1_filters)


    '''for fully output layers: (input_features, output_features)'''
    # layer 7
    # output layer: 10
    self.output_size = 10
    self.output_weights = np.random.randn(self.num_fc1_filters, self.output_size) * 0.1
    self.output_bias = np.zeros(self.output_size)



  '''forward pass'''
  def forward(self, X):
    '''accepts only input image i.e. (1, 32,32)'''


    # to store inputs and outputs for the backprop(PITA)
    self.cache = {} # this will store the intermediate conv operation results (flow of the input from init to final layer)
    self.cache['X'] = X # this is the

    # Block 1
    # layer 1: conv : (32x32 -> 28x28 @ 6 filters of size 5x5)
    conv1_output = self.convolution (X, self.conv1_weights, self.conv1_bias, self.conv1_kernel_size)
    self.cache['conv1_output'] = conv1_output
    conv1_activated = self.tanh(conv1_output)
    self.cache['conv1_activated'] = conv1_activated

    # layer 2: pooling : takes 28x28 -> 14x14 @ 6 filters of size 2x2
    pool1_output = self.pooling(conv1_activated, self.pool1_size, self.pool1_weights, self.pool1_bias)
    self.cache['pool1_output'] = pool1_output
    pool1_activated = self.tanh(pool1_output)
    self.cache['pool1_activated'] = pool1_activated

    # Block 2
    # layer 3: conv: 14x14 -> 10 x10 @16 filters of size 5x5
    conv2_output = self.convolution(pool1_activated, self.conv2_weights, self.conv2_bias, self.conv2_kernel_size)
    self.cache['conv2_output'] = conv2_output
    conv2_activated = self.tanh(conv2_output)
    self.cache['conv2_activated'] = conv2_activated

    # layer 4: pooling: takes 10x10 -> 5x5 @ 16 filters of size 2x2
    pool2_output = self.pooling(conv2_activated, self.pool2_size, self.pool2_weights, self.pool2_bias)
    self.cache['pool2_output'] = pool2_output
    pool2_activated = self.tanh(pool2_output)
    self.cache['pool2_activated'] = pool2_activated


    # Block 3
    # layer 5: conv(FC) : 5x5 -> 1x1 @ 120 filters of size 5x5
    conv3_output = self.convolution(pool2_activated, self.conv3_weights, self.conv3_bias, self.conv3_kernel_size)
    self.cache['conv3_output'] = conv3_output
    conv3_activated = self.tanh(conv3_output)
    self.cache['conv3_activated'] = conv3_activated


    # # NOT USED IN THE ORIGNAL PAPER:Now flattening : making it 1-D
    # flattened = self.flatten(conv3_activated)
    # self.cache['flattened'] = flattened

    # block 4
    # layer 6: 120 -> 84

    fc_input  = conv3_activated.reshape(-1) # kinda making it 1d by reshaping
    self.cache['fc_input'] = fc_input

    fc1_output = np.dot(fc_input, self.fc1_weights) + self.fc1_bias
    self.cache['fc1_output'] = fc1_output
    fc1_activated = self.tanh(fc1_output)
    self.cache['fc1_activated'] = fc1_activated

    # final block
    # output layer

    rbf_output = np.zeros(self.output_size)
    for i in range(self.output_size):

      distance = np.sum((fc1_activated - self.output_weights[:, i]) ** 2) # coming from equation 7 in the paper

      rbf_output[i] = -np.exp(-0.5 * distance)

    self.cache['rbf_output'] = rbf_output
    print(f'rbf output: {rbf_output}')

    return rbf_output






  def convolution(self, X, weight, b, kernel_size ):
    '''
    X: input shape (no. of channels, height, width)
    weights: output filters, input, kernal_height, kernel_width
    bias = bias shape (output_fitlers)
    kernal: sq. sized

    returns: output_filters, output_height and width( calculated on the fly during operation)'''



    in_filters, in_h, in_w = X.shape # this will fetch the channels(in case of colored images and height/width)

    out_filters = weight.shape[0] # determines the shape of the output after conv operation
    out_h = in_h - kernel_size + 1
    out_w = in_w - kernel_size + 1

    # empty 'canvas' for the output from the convolution
    conv_output = np.zeros((out_filters, out_h, out_w))

    ''' very imp stuff now i.e. the GOAT conv'''

    for o_f in range (out_filters): # for number of filters in the expected output after conv
      for i_f in range(in_filters): # for number of the filters coming from the input
        # iterating over the empty canvas row or the expected output to fill the values in
        for h in range(out_h):
          for w in range(out_w):
            # kinda making window rn t that will convolve
            window = X[i_f, h:h+kernel_size, w:w+kernel_size]

            # now performing elementwise multiplication and sum i.e. actual convolution
            conv_output[o_f, h, w] += np.sum (window * weight[o_f,i_f])
      # adding bias to the convolution
      conv_output[o_f] += b[o_f]

    return conv_output

  def pooling(self, X, pool_size, weight, b):
    filters, in_h, in_w = X.shape
    out_h, out_w = in_h // pool_size, in_w // pool_size # consider the size of the pool is 2 then as we are averaging so th eoutput size should be half of hte input
    pool_output  = np.zeros((filters, out_h, out_w))

    # now pooling

    for filter in range(filters): # number of feature maps after this
      for h in range(out_h): # Expected number of cols in out feature map
        for w in range(out_w): # expexted number of the rows in the out feature map
          h_start = h * pool_size # as each time we take 2 cells (in this case) so we have to start with +2 cells nxt time

          w_start = w * pool_size # as each time we take 2 cells (in this case) so we have to start with +2 cells nxt time

          window = X [filter, h_start:h_start + pool_size, w_start: w_start + pool_size]

          # meaning the selected window
          pool_output [filter, h, w] = np.mean(window) * weight[filter] + b[filter]
    return pool_output

  def tanh(self,X):
    return np.tanh(X)

  def predict(self, X):
    rbf_outputs = self.forward(X)
    # it is like in case of radial basis function we take index of the most small (even negative value)
    return np.argmin(rbf_outputs)


  ''' Functions for backprop'''

  def tanh_derivative(self, x):
    return 1.0 - np.tanh(x) ** 2

  def compute_loss(self, y_pred, y_true):
    """
    Compute the loss based on predicted outputs and true label
    """
    # Convert y_true to one-hot encoding
    y_one_hot = np.zeros(self.output_size)
    y_one_hot[y_true] = 1

    # Mean squared error between RBF outputs and one-hot encoding
    loss = np.mean((y_pred - y_one_hot)**2)
    return loss

  ''' generated with the AI as out of scope '''
  def backpropagation(self, y_true):
    """
    Compute gradients for all parameters via backpropagation
    """
    # Initialize gradients
    dconv1_w = np.zeros_like(self.conv1_weights)
    dconv1_b = np.zeros_like(self.conv1_bias)
    dpool1_w = np.zeros_like(self.pool1_weights)
    dpool1_b = np.zeros_like(self.pool1_bias)
    dconv2_w = np.zeros_like(self.conv2_weights)
    dconv2_b = np.zeros_like(self.conv2_bias)
    dpool2_w = np.zeros_like(self.pool2_weights)
    dpool2_b = np.zeros_like(self.pool2_bias)
    dconv3_w = np.zeros_like(self.conv3_weights)
    dconv3_b = np.zeros_like(self.conv3_bias)
    dfc1_w = np.zeros_like(self.fc1_weights)
    dfc1_b = np.zeros_like(self.fc1_bias)
    doutput_w = np.zeros_like(self.output_weights)
    doutput_b = np.zeros_like(self.output_bias)

    # Convert y_true to one-hot encoding
    y_one_hot = np.zeros(self.output_size)
    y_one_hot[y_true] = 1

    # Backpropagation for output layer (RBF)
    rbf_output = self.cache['rbf_output']
    fc1_activated = self.cache['fc1_activated']

    # Gradient of loss with respect to RBF output
    dL_drbf = 2 * (rbf_output - y_one_hot) / self.output_size

    # Gradient for RBF layer weights
    for i in range(self.output_size):
        if dL_drbf[i] != 0:  # Only compute if there's a gradient to propagate
            diff = fc1_activated - self.output_weights[:, i]
            drbf_dfc1 = -np.exp(-0.5 * np.sum(diff**2)) * (-diff)
            doutput_w[:, i] += dL_drbf[i] * drbf_dfc1

    # Gradient for FC1 layer
    dL_dfc1_act = np.zeros_like(fc1_activated)
    for i in range(self.output_size):
        if dL_drbf[i] != 0:
            diff = fc1_activated - self.output_weights[:, i]
            drbf_dfc1 = -np.exp(-0.5 * np.sum(diff**2)) * (-diff)
            dL_dfc1_act += dL_drbf[i] * drbf_dfc1

    # Gradient through tanh activation
    fc1_output = self.cache['fc1_output']
    dfc1_act_dfc1 = self.tanh_derivative(fc1_output)
    dL_dfc1 = dL_dfc1_act * dfc1_act_dfc1

    # Gradient for FC1 weights and bias
    fc_input = self.cache['fc_input']
    dfc1_w = np.outer(fc_input, dL_dfc1)
    dfc1_b = dL_dfc1

    # Gradient for Conv3 layer
    dL_dfc_input = np.dot(dL_dfc1, self.fc1_weights.T)
    dL_dconv3_act = dL_dfc_input.reshape(self.num_conv3_filters, 1, 1)

    # Gradient through tanh activation
    conv3_output = self.cache['conv3_output']
    dconv3_act_dconv3 = self.tanh_derivative(conv3_output)
    dL_dconv3 = dL_dconv3_act * dconv3_act_dconv3

    # Gradient for Conv3 weights and biases
    pool2_activated = self.cache['pool2_activated']
    for o_f in range(self.num_conv3_filters):
        for i_f in range(self.num_conv2_filters):
            for h in range(dL_dconv3.shape[1]):
                for w in range(dL_dconv3.shape[2]):
                    h_start = h
                    h_end = h + self.conv3_kernel_size
                    w_start = w
                    w_end = w + self.conv3_kernel_size

                    if h_end <= pool2_activated.shape[1] and w_end <= pool2_activated.shape[2]:
                        window = pool2_activated[i_f, h_start:h_end, w_start:w_end]
                        dconv3_w[o_f, i_f] += dL_dconv3[o_f, h, w] * window

        dconv3_b[o_f] += np.sum(dL_dconv3[o_f])

    # Gradient for Pool2 layer
    dL_dpool2_act = np.zeros_like(pool2_activated)

    # Propagate gradients from Conv3 to Pool2
    for o_f in range(self.num_conv3_filters):
        for i_f in range(self.num_conv2_filters):
            for h in range(dL_dconv3.shape[1]):
                for w in range(dL_dconv3.shape[2]):
                    h_start = h
                    h_end = h + self.conv3_kernel_size
                    w_start = w
                    w_end = w + self.conv3_kernel_size

                    if h_end <= pool2_activated.shape[1] and w_end <= pool2_activated.shape[2]:
                        dL_dpool2_act[i_f, h_start:h_end, w_start:w_end] += dL_dconv3[o_f, h, w] * self.conv3_weights[o_f, i_f]

    # Gradient through tanh activation
    pool2_output = self.cache['pool2_output']
    dpool2_act_dpool2 = self.tanh_derivative(pool2_output)
    dL_dpool2 = dL_dpool2_act * dpool2_act_dpool2

    # Gradient for Pool2 weights and biases
    conv2_activated = self.cache['conv2_activated']
    for f in range(self.num_conv2_filters):
        for h in range(dL_dpool2.shape[1]):
            for w in range(dL_dpool2.shape[2]):
                h_start = h * self.pool2_size
                h_end = h_start + self.pool2_size
                w_start = w * self.pool2_size
                w_end = w_start + self.pool2_size

                window = conv2_activated[f, h_start:h_end, w_start:w_end]
                dpool2_w[f] += np.mean(window) * dL_dpool2[f, h, w]
                dpool2_b[f] += dL_dpool2[f, h, w]

    # Gradient for Conv2 layer
    dL_dconv2_act = np.zeros_like(conv2_activated)

    # Propagate gradients from Pool2 to Conv2
    for f in range(self.num_conv2_filters):
        for h in range(dL_dpool2.shape[1]):
            for w in range(dL_dpool2.shape[2]):
                h_start = h * self.pool2_size
                h_end = h_start + self.pool2_size
                w_start = w * self.pool2_size
                w_end = w_start + self.pool2_size

                dL_dconv2_act[f, h_start:h_end, w_start:w_end] += dL_dpool2[f, h, w] * self.pool2_weights[f] / (self.pool2_size * self.pool2_size)

    # Gradient through tanh activation
    conv2_output = self.cache['conv2_output']
    dconv2_act_dconv2 = self.tanh_derivative(conv2_output)
    dL_dconv2 = dL_dconv2_act * dconv2_act_dconv2

    # Gradient for Conv2 weights and biases
    pool1_activated = self.cache['pool1_activated']

    for o_f in range(self.num_conv2_filters):
        for i_f in range(self.num_conv1_filters):
            for h in range(dL_dconv2.shape[1]):
                for w in range(dL_dconv2.shape[2]):
                    h_start = h
                    h_end = h + self.conv2_kernel_size
                    w_start = w
                    w_end = w + self.conv2_kernel_size

                    if h_end <= pool1_activated.shape[1] and w_end <= pool1_activated.shape[2]:
                        window = pool1_activated[i_f, h_start:h_end, w_start:w_end]
                        dconv2_w[o_f, i_f] += dL_dconv2[o_f, h, w] * window

        dconv2_b[o_f] += np.sum(dL_dconv2[o_f])

    # Gradient for Pool1 layer
    dL_dpool1_act = np.zeros_like(pool1_activated)

    # Propagate gradients from Conv2 to Pool1
    for o_f in range(self.num_conv2_filters):
        for i_f in range(self.num_conv1_filters):
            for h in range(dL_dconv2.shape[1]):
                for w in range(dL_dconv2.shape[2]):
                    h_start = h
                    h_end = h + self.conv2_kernel_size
                    w_start = w
                    w_end = w + self.conv2_kernel_size

                    if h_end <= pool1_activated.shape[1] and w_end <= pool1_activated.shape[2]:
                        dL_dpool1_act[i_f, h_start:h_end, w_start:w_end] += dL_dconv2[o_f, h, w] * self.conv2_weights[o_f, i_f]

    # Gradient through tanh activation
    pool1_output = self.cache['pool1_output']
    dpool1_act_dpool1 = self.tanh_derivative(pool1_output)
    dL_dpool1 = dL_dpool1_act * dpool1_act_dpool1

    # Gradient for Pool1 weights and biases
    conv1_activated = self.cache['conv1_activated']
    for f in range(self.num_conv1_filters):
        for h in range(dL_dpool1.shape[1]):
            for w in range(dL_dpool1.shape[2]):
                h_start = h * self.pool1_size
                h_end = h_start + self.pool1_size
                w_start = w * self.pool1_size
                w_end = w_start + self.pool1_size

                window = conv1_activated[f, h_start:h_end, w_start:w_end]
                dpool1_w[f] += np.mean(window) * dL_dpool1[f, h, w]
                dpool1_b[f] += dL_dpool1[f, h, w]

    # Gradient for Conv1 layer
    dL_dconv1_act = np.zeros_like(conv1_activated)

    # Propagate gradients from Pool1 to Conv1
    for f in range(self.num_conv1_filters):
        for h in range(dL_dpool1.shape[1]):
            for w in range(dL_dpool1.shape[2]):
                h_start = h * self.pool1_size
                h_end = h_start + self.pool1_size
                w_start = w * self.pool1_size
                w_end = w_start + self.pool1_size

                dL_dconv1_act[f, h_start:h_end, w_start:w_end] += dL_dpool1[f, h, w] * self.pool1_weights[f] / (self.pool1_size * self.pool1_size)

    # Gradient through tanh activation
    conv1_output = self.cache['conv1_output']
    dconv1_act_dconv1 = self.tanh_derivative(conv1_output)
    dL_dconv1 = dL_dconv1_act * dconv1_act_dconv1

    # Gradient for Conv1 weights and biases
    X = self.cache['X']

    for o_f in range(self.num_conv1_filters):
        for i_f in range(X.shape[0]):  # Input channels
            for h in range(dL_dconv1.shape[1]):
                for w in range(dL_dconv1.shape[2]):
                    h_start = h
                    h_end = h + self.conv1_kernel_size
                    w_start = w
                    w_end = w + self.conv1_kernel_size

                    if h_end <= X.shape[1] and w_end <= X.shape[2]:
                        window = X[i_f, h_start:h_end, w_start:w_end]
                        dconv1_w[o_f, i_f] += dL_dconv1[o_f, h, w] * window

        dconv1_b[o_f] += np.sum(dL_dconv1[o_f])

    # Checking the updates
    # print(f"Conv1 gradient magnitude: {np.mean(np.abs(dconv1_w))}")
    # print(f"FC1 gradient magnitude: {np.mean(np.abs(dfc1_w))}")
    # print(f"Output gradient magnitude: {np.mean(np.abs(doutput_w))}")
    # Apply all gradients using learning rate
    self.conv1_weights -= self.lr * dconv1_w
    self.conv1_bias -= self.lr * dconv1_b
    self.pool1_weights -= self.lr * dpool1_w
    self.pool1_bias -= self.lr * dpool1_b
    self.conv2_weights -= self.lr * dconv2_w
    self.conv2_bias -= self.lr * dconv2_b
    self.pool2_weights -= self.lr * dpool2_w
    self.pool2_bias -= self.lr * dpool2_b
    self.conv3_weights -= self.lr * dconv3_w
    self.conv3_bias -= self.lr * dconv3_b
    self.fc1_weights -= self.lr * dfc1_w
    self.fc1_bias -= self.lr * dfc1_b
    self.output_weights -= self.lr * doutput_w
    self.output_bias -= self.lr * doutput_b

    return self.compute_loss(rbf_output, y_true)

  def train(self, X_train, y_train, batch_size = 1):
    '''params to identify
    X_train: num_samples, channels, h, w - > each sample is still good to be accessed with the indexing
    y_train: num_samples -<> labels
    batch_size: for mini-batch gradient descent'''

    num_samples = X_train.shape[0] # getting total number of samples
    losses = [] # to accumulate losses

    for epoch in range(self.epochs): # how many times we are looking to train the model?
      epoch_loss = 0
      indices = np.random.permutation(num_samples) #randomizing indices and shuffeling the indices for that
      X_shuffled, y_shuffled = X_train[indices], y_train[indices]


      # for each mini batch
      for i in range(0, num_samples, batch_size): # stepping by the total samples in the batch for whole epoch
        X_batch = X_shuffled[i :  i + batch_size]
        y_batch = y_shuffled[i :  i + batch_size]

        batch_loss = 0 # accumulating losses for each batch

        # now for each sample
        for j in range (len(X_batch)):
          # image goes in the forward pass secttion
          self.forward(X_batch[j])

          # loss calculation and backprop
          sample_loss = self.backpropagation(y_batch[j])
          batch_loss += sample_loss # acccumulate loss

        # mean of the loss for each batch
        batch_loss /= len(X_batch)
        epoch_loss += batch_loss
        print(f'\n\n\nEpoch {epoch+1}, Batch {i//batch_size + 1}: {batch_loss:.4f}\n\n\n')

      # Averging for the epochs
      epoch_loss /= (num_samples // batch_size)
      losses.append(epoch_loss)

      # print(losses[-1])

      # printing progress

      if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss:.4f}")
    return losses


  def evaluate(self, X_test, y_test):
    """
    Evaluate the model on test data

    Parameters:
    X_test: Test images, shape (num_samples, channels, height, width)
    y_test: Test labels, shape (num_samples,)

    Returns:
    accuracy: Classification accuracy
    """
    correct = 0
    for i in range(len(X_test)):
        pred = self.predict(X_test[i])
        if pred == y_test[i]:
            correct += 1

    accuracy = correct / len(X_test)
    return accuracy




'''driving point '''
import numpy as np
from sklearn.datasets import fetch_openml

# Load MNIST
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist["data"].to_numpy(), mnist["target"].astype(int).to_numpy() # input and target

# Preprocess data (reshape to proper format, normalize)
def prepare_mnist_data(X, y, num_samples=1000):
    # subset for the faster testing
    X = X[:num_samples]
    y = y[:num_samples]

    # Reshape to (n_samples, 1, 28, 28) - 1 channel for grayscale
    X = X.reshape(-1, 1, 28, 28) # -1: infer the dimension on its own, 1: grayscale, 28x28 : height and width

    # normalization
    X = X.astype(np.float32) / 255.0

    # so LeNet implenentaiont that is why 32x32 is padded to 28x28
    padded_X = np.zeros((X.shape[0], 1, 32, 32), dtype=np.float32)
    for i in range(X.shape[0]):
        padded_X[i, 0, 2:30, 2:30] = X[i, 0]

    return padded_X, y

# Process data
X_processed, y_processed = prepare_mnist_data(X, y)

# Split into train and test : taking only 800 samples
X_train, y_train = X_processed[:800], y_processed[:800]
X_test, y_test = X_processed[800:], y_processed[800:]

# Model creation and traning
model = lenet5()
losses = model.train(X_train, y_train, batch_size=32)

# Test
accuracy = model.evaluate(X_test, y_test)
print(f"Test accuracy: {accuracy}")

